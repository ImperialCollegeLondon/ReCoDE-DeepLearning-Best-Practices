{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn PyTorch Lightning: A Lightweight PyTorch Wrapper\n",
    "\n",
    "PyTorch Lightning is a lightweight PyTorch wrapper that helps organize your PyTorch code, making it more readable and maintainable. You can read about how to convert your PyTorch code: https://lightning.ai/docs/pytorch/stable/starter/converting.html. It abstracts away much of the boilerplate code, allowing you to focus on the core logic of your models. This tutorial will guide you through the basics of PyTorch Lightning.\n",
    "\n",
    "## 1. Introduction to PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning separates the research code from the engineering code, helping you write scalable and more readable code. It automates most of the training loop and other common functionalities, making it easier to replicate results and scale your projects.\n",
    "\n",
    "## 2. Installing PyTorch Lightning\n",
    "\n",
    "Before you begin, you need to have PyTorch installed. Then, install PyTorch Lightning via pip:\n",
    "\n",
    "```bash\n",
    "pip install pytorch-lightning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Lightning Module\n",
    "\n",
    "A Lightning Module is where you define your model, just like a standard PyTorch `nn.Module`, but you also define the training step, validation step, etc. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x.view(x.size(0), -1))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "\n",
    "    # Uncomment to add validation step\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     x, y = batch\n",
    "    #     logits = self(x)\n",
    "    #     loss = F.cross_entropy(logits, y)\n",
    "    #     # Add logging\n",
    "    #     self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "model = LitModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "PyTorch Lightning works with the standard PyTorch DataLoader. Let's load the MNIST dataset as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST(\"\", train=True, download=True, transform=transforms.ToTensor()), batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "# Check data\n",
    "x, y = next(iter(train_loader))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "\n",
    "Training a model with PyTorch Lightning is straightforward. You just need to initialize a `Trainer` and call the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our model\n",
    "model = LitModel()\n",
    "print(model)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(max_epochs=3)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation and Testing\n",
    "\n",
    "You can easily add validation and test steps in your `LitModel`. For validation, implement the `validation_step` method:\n",
    "\n",
    "```python\n",
    "class LitModel(pl.LightningModule):\n",
    "    # ...\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        # Add logging\n",
    "        self.log('val_loss', loss)\n",
    "```\n",
    "\n",
    "Use a similar approach for the `test_step`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logging and Callbacks\n",
    "\n",
    "PyTorch Lightning comes with built-in support for logging and callbacks. You can use TensorBoard, or other loggers like MLFlow, Comet, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", dirpath=\"./my_model\", filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=3, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Features\n",
    "\n",
    "PyTorch Lightning also supports distributed training, mixed precision training, and more. These features can be easily activated in the Trainer (make sure you have the required hardware and software installed). Here for example, we would start training on two GPUs and use mixed precision training:\n",
    "\n",
    "```python\n",
    "trainer = pl.Trainer(gpus=2, precision=16)\n",
    "```\n",
    "\n",
    "If you don't know the number of GPUs available, you can set `gpus=-1` and PyTorch Lightning will automatically use all available GPUs.\n",
    "\n",
    "For more options on the `Trainer`, check out the [documentation](https://lightning.ai/docs/pytorch/stable/common/trainer.html).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "PyTorch Lightning is a powerful tool for organizing PyTorch code and making it more efficient and maintainable. It abstracts away the engineering details, allowing you to focus on the research part. This tutorial covered the basics, but there's a lot more to explore, including advanced features like distributed training, and integrations with other tools and libraries. Be sure to check out the [official documentation](https://www.pytorchlightning.ai/) for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
