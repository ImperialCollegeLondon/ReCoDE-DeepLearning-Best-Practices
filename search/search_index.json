{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#deep-learning-best-practices","title":"Deep Learning Best Practices","text":""},{"location":"#description","title":"Description","text":"<p>This project aims to showcase best practices and tools essential for initiating a successful deep learning project. It will cover the use of configuration files for project settings management, adopting a modular code architecture, and utilizing frameworks like Hydra for efficient configuration. The project will also focus on effective result logging and employing templates for project structuring, aiding in maintainability, scalability, and collaborative ease.</p>"},{"location":"#learning-outcomes","title":"Learning outcomes","text":"<ol> <li>Using Wandb to Log Training Metrics and Images</li> <li> <p>Master the integration of Wandb (Weights &amp; Biases) into your project for comprehensive logging of training metrics and images. This includes setting up Wandb, configuring it for your project, and utilizing its powerful dashboard for real-time monitoring and analysis of model performance.</p> </li> <li> <p>Using Hydra to Manage Configurations</p> </li> <li> <p>Learn to leverage Hydra for advanced configuration management in your projects. Understand how to define, organize, and override configurations dynamically, enabling flexible experimentation and streamlined management of complex projects.</p> </li> <li> <p>Using PyTorch Lightning to Train Models</p> </li> <li> <p>Gain expertise in using PyTorch Lightning to simplify the training of machine learning models. This includes setting up models, training loops, validation, testing, and leveraging PyTorch Lightning's abstractions for cleaner, more maintainable code.</p> </li> <li> <p>Einops for Easy Tensor Manipulation</p> </li> <li> <p>Acquire the skills to use Einops for intuitive and efficient tensor operations, enhancing the readability and scalability of your data manipulation code. Learn to apply Einops for reshaping, repeating, and rearranging tensors in a more understandable way.</p> </li> <li> <p>Learning to Start Training on GPU</p> </li> <li> <p>Understand how to utilize GPUs for training your models. This outcome covers the basics of GPU acceleration, including how to select and allocate GPU resources for your training jobs to improve computational efficiency.</p> </li> <li> <p>Using a Template to Start Project</p> </li> <li>Familiarize yourself with starting new projects using a predefined template, specifically the Minimal Lightning Hydra Template. Learn the benefits of using templates for project initialization, including predefined directory structures, configuration files, and sample code to kickstart your development process.</li> </ol>"},{"location":"#getting-familiar-with-the-libraries","title":"Getting Familiar with the Libraries","text":"<p>To help you become acquainted with the key libraries used in this project, I have prepared learning notebooks that cover the basics of each. These interactive notebooks are an excellent way to get hands-on experience. You can find them in the learning folder. The notebooks are designed to introduce you to the core concepts and functionalities of each library:</p> <ul> <li>Pytorch Lightning: This notebook introduces Pytorch Lightning, a library that simplifies the training process of PyTorch models. It covers basic concepts like creating models, training loops, and leveraging Lightning's built-in functionalities for more efficient training.</li> <li>Hydra: Hydra is a framework for elegantly configuring complex applications. This notebook will guide you through its configuration management capabilities, demonstrating how to streamline your project's settings and parameters.</li> <li>Einops: Einops is a library for tensor operations and manipulation. Learn how to use Einops for more readable and maintainable tensor transformations in this notebook.</li> </ul> <p>For a more comprehensive understanding, I also recommend the following tutorials. They provide in-depth knowledge and are great resources for both beginners and experienced users:</p> <ul> <li>Pytorch Lightning Tutorial: An official guide to starting a new project with Pytorch Lightning, offering step-by-step instructions and best practices.</li> <li>Hydra Documentation: The official introduction to Hydra, covering its core principles and how to integrate it into your applications.</li> <li>Wandb Quickstart: A quickstart guide for Wandb, a tool for experiment tracking, visualization, and comparison. Learn how to integrate Wandb into your machine learning projects.</li> <li>Einops Basics: An introductory tutorial to Einops, focusing on the basics and fundamental concepts of the library.</li> </ul> <p>By exploring these notebooks and tutorials, you will gain a solid foundation in these libraries, which are integral to the project's development.</p>"},{"location":"#starting-a-new-project","title":"Starting a New Project","text":"<p>To main steps to start a new project are described here. This notebook will guide you through the process of initializing a new project and will showcase the best practices and tools used in this project.</p>"},{"location":"#further-learning","title":"Further Learning","text":"<p>This project is designed to provide a comprehensive understanding of best practices in deep learning, incorporating the use of PyTorch, PyTorch Lightning, Hydra, and other essential tools. However, the field of deep learning is vast and constantly evolving. To continue your learning journey, I recommend exploring the following resources:</p> <ul> <li> <p>Deep Learning Tuning Playbook: A detailed guide focused on maximizing the performance of deep learning models. It covers aspects of deep learning training such as pipeline implementation, optimization, and hyperparameter tuning.</p> </li> <li> <p>PyTorch Performance Tuning: This resource provides a set of optimizations and best practices that can accelerate the training and inference of deep learning models in PyTorch. I also recommend exploring the official PyTorch documentation, which is a rich source of tutorials, guides, and examples.</p> </li> <li> <p>Lightning Training Tricks: PyTorch Lightning implements various techniques to aid in training, making the process more efficient and smoother.</p> </li> <li> <p>Scientific Software Best Practices: This exemplar project showcases best practices in developing scientific software, offering insights into structured project management.</p> </li> <li> <p>Further Tools: The deep learning ecosystem includes many other tools and libraries that can enhance your projects. For instance, Fire for automatically generating command line interfaces, DeepSpeed for efficient distributed training and inference, and Optuna for advanced hyperparameter optimization.</p> </li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<p>The directory structure of new project looks like this:</p> <pre><code>\u251c\u2500\u2500 .github                   &lt;- Github Actions workflows\n\u2502\n\u251c\u2500\u2500 configs                   &lt;- Hydra configs\n\u2502   \u251c\u2500\u2500 callbacks                &lt;- Callbacks configs\n\u2502   \u251c\u2500\u2500 datamodule               &lt;- Data configs\n\u2502   \u251c\u2500\u2500 debug                    &lt;- Debugging configs\n\u2502   \u251c\u2500\u2500 experiment               &lt;- Experiment configs\n\u2502   \u251c\u2500\u2500 extras                   &lt;- Extra utilities configs\n\u2502   \u251c\u2500\u2500 hparams_search           &lt;- Hyperparameter search configs\n\u2502   \u251c\u2500\u2500 hydra                    &lt;- Hydra configs\n\u2502   \u251c\u2500\u2500 local                    &lt;- Local configs\n\u2502   \u251c\u2500\u2500 logger                   &lt;- Logger configs\n\u2502   \u251c\u2500\u2500 model                    &lt;- Model configs\n\u2502   \u251c\u2500\u2500 paths                    &lt;- Project paths configs\n\u2502   \u251c\u2500\u2500 trainer                  &lt;- Trainer configs\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 eval.yaml             &lt;- Main config for evaluation\n\u2502   \u2514\u2500\u2500 train.yaml            &lt;- Main config for training\n\u2502   \u2514\u2500\u2500 inference.yaml        &lt;- Main config for inference\n\u2502\n\u251c\u2500\u2500 data                   &lt;- Project data\n\u2502\n\u251c\u2500\u2500 logs                   &lt;- Logs generated by hydra and lightning loggers\n\u2502\n\u251c\u2500\u2500 notebooks              &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n\u2502                             the creator's initials, and a short `-` delimited description,\n\u2502                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.\n\u2502\n\u251c\u2500\u2500 scripts                &lt;- Shell scripts\n\u2502\n\u251c\u2500\u2500 src                    &lt;- Source code\n\u2502   \u251c\u2500\u2500 datamodules              &lt;- Data scripts\n\u2502   \u251c\u2500\u2500 models                   &lt;- Model scripts\n\u2502   \u251c\u2500\u2500 utils                    &lt;- Utility scripts\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 eval.py                  &lt;- Run evaluation\n\u2502   \u251c\u2500\u2500 train.py                 &lt;- Run training\n\u2502   \u251c\u2500\u2500 inference.py             &lt;- Run inference\n\u2502\n\u251c\u2500\u2500 tests                  &lt;- Tests of any kind\n\u2502\n\u251c\u2500\u2500 .env.example              &lt;- Example of file for storing private environment variables\n\u251c\u2500\u2500 .gitignore                &lt;- List of files ignored by git\n\u251c\u2500\u2500 .pre-commit-config.yaml   &lt;- Configuration of pre-commit hooks for code formatting\n\u251c\u2500\u2500 .project-root             &lt;- File for inferring the position of project root directory\n\u251c\u2500\u2500 environment.yaml          &lt;- File for installing conda environment\n\u251c\u2500\u2500 Makefile                  &lt;- Makefile with commands like `make train` or `make test`\n\u251c\u2500\u2500 pyproject.toml            &lt;- Configuration options for creating python package\n\u251c\u2500\u2500 requirements.txt          &lt;- File for installing python dependencies\n\u2514\u2500\u2500 README.md\n</code></pre> <p>You will find an extra folder called <code>docs</code> that contains a collection of markdown files with best practices and explanations of how to use the project.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>"},{"location":"learning/Additional_Best_Practices/","title":"Additional Best Practices","text":"Use Miniconda  It's usually unnecessary to install full anaconda environment, miniconda should be enough (weights around 80MB).  Big advantage of conda is that it allows for installing packages without requiring certain compilers or libraries to be available in the system (since it installs precompiled binaries), so it often makes it easier to install some dependencies e.g. cudatoolkit for GPU support.  It also allows you to access your environments globally which might be more convenient than creating new local environment for every project.  Example installation:  <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>  Update conda:  <pre><code>conda update -n base -c defaults conda\n</code></pre>  Create new conda environment:  <pre><code>conda create -n myenv python=3.10\nconda activate myenv\n</code></pre> Set private environment variables in .env file  System specific variables (e.g. absolute paths to datasets) should not be under version control or it will result in conflict between different users. Your private keys also shouldn't be versioned since you don't want them to be leaked.  Template contains `.env.example` file, which serves as an example. Create a new file called `.env` (this name is excluded from version control in .gitignore). You should use it for storing environment variables like this:  <pre><code>MY_VAR=/home/user/my_system_path\n</code></pre>  All variables from `.env` are loaded in `train.py` automatically.  Hydra allows you to reference any env variable in `.yaml` configs like this:  <pre><code>path_to_data: ${oc.env:MY_VAR}\n</code></pre> Name metrics using '/' character  Depending on which logger you're using, it's often useful to define metric name with `/` character:  <pre><code>self.log(\"train/loss\", loss)\n</code></pre>  This way loggers will treat your metrics as belonging to different sections, which helps to get them organised in UI.   Use torchmetrics  Use official [torchmetrics](https://github.com/PytorchLightning/metrics) library to ensure proper calculation of metrics. This is especially important for multi-GPU training!  For example, instead of calculating accuracy by yourself, you should use the provided `Accuracy` class like this:  <pre><code>from torchmetrics.classification.accuracy import Accuracy\n\n\nclass LitModel(LightningModule):\n    def __init__(self)\n        self.train_acc = Accuracy()\n        self.val_acc = Accuracy()\n\n    def training_step(self, batch, batch_idx):\n        ...\n        acc = self.train_acc(predictions, targets)\n        self.log(\"train/acc\", acc)\n        ...\n\n    def validation_step(self, batch, batch_idx):\n        ...\n        acc = self.val_acc(predictions, targets)\n        self.log(\"val/acc\", acc)\n        ...\n</code></pre>  Make sure to use different metric instance for each step to ensure proper value reduction over all GPU processes.  Torchmetrics provides metrics for most use cases, like F1 score or confusion matrix. Read [documentation](https://torchmetrics.readthedocs.io/en/latest/#more-reading) for more.   Follow PyTorch Lightning style guide  The style guide is available [here](https://pytorch-lightning.readthedocs.io/en/latest/starter/style_guide.html).  1. Be explicit in your init. Try to define all the relevant defaults so that the user doesn\u2019t have to guess. Provide type hints. This way your module is reusable across projects!     <pre><code>class LitModel(LightningModule):\n    def __init__(self, layer_size: int = 256, lr: float = 0.001):\n</code></pre>  2. Preserve the recommended method order.     <pre><code>class LitModel(LightningModule):\n\n    def __init__():\n        ...\n\n    def forward():\n        ...\n\n    def training_step():\n        ...\n\n    def training_step_end():\n        ...\n\n    def on_train_epoch_end():\n        ...\n\n    def validation_step():\n        ...\n\n    def validation_step_end():\n        ...\n\n    def on_validation_epoch_end():\n        ...\n\n    def test_step():\n        ...\n\n    def test_step_end():\n        ...\n\n    def on_test_epoch_end():\n        ...\n\n    def configure_optimizers():\n        ...\n\n    def any_extra_hook():\n        ...\n</code></pre> Use Tmux  Tmux is a terminal multiplexer, which allows you to run multiple terminal sessions in a single window. It's especially useful when you want to run your training script on a remote server and you want to keep it running even after you close the ssh connection.  More about tmux can be found here.  Specify the GPU device  When running your script on a server with multiple GPUs, you should specify which GPU to use. You can do this by setting the `CUDA_VISIBLE_DEVICES` environment variable:  <pre><code>CUDA_VISIBLE_DEVICES=0 python train.py\n</code></pre>  This will make sure that your script uses only the first GPU. If you want to use multiple GPUs, you can specify them like this:  <pre><code>CUDA_VISIBLE_DEVICES=0,1 python train.py\n</code></pre>"},{"location":"learning/Install_as_package/","title":"Instal Repository as a Package (Optional)","text":"<p>Creating a Python package involves organizing your code into a structured format that can be easily distributed and installed. Utilizing <code>pyproject.toml</code> is a modern approach that specifies build system requirements for Python projects. Here's a small tutorial on how to transform the given repository into a Python package using <code>pyproject.toml</code>.</p> <p>This is a minimal example of how to create a Python package. You can find more details at the Python Packaging User Guide or using this template.</p>"},{"location":"learning/Install_as_package/#step-1-organize-your-code","title":"Step 1: Organize Your Code","text":"<p>First, ensure your project has a suitable structure. A typical package structure looks like this:</p> <pre><code>your_project_name/\n\u2502\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 your_package_name/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 module1.py\n\u2502       \u2514\u2500\u2500 module2.py\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_module1.py\n\u2502   \u2514\u2500\u2500 test_module2.py\n\u2502\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 README.md\n</code></pre> <ul> <li><code>your_project_name/</code> is the root directory.</li> <li><code>src/</code> contains all your source files. It's a good practice to keep your package code inside a <code>src/</code> directory to avoid import issues.</li> <li><code>your_package_name/</code> is the directory that will be the actual Python package.</li> <li><code>tests/</code> contains your unit tests.</li> <li><code>pyproject.toml</code> will contain your package metadata and build requirements.</li> </ul> <p>As you see the current structure needs to be slightly modified to fit the package structure. The only change is to move everything from the <code>src</code> folder to <code>src/your_package_name</code>. This is the only change needed to make the project a package.</p>"},{"location":"learning/Install_as_package/#step-2-create-the-pyprojecttoml-file","title":"Step 2: Create the <code>pyproject.toml</code> File","text":"<p>The <code>pyproject.toml</code> file is a configuration file to define your package metadata and build system requirements. Explanation of the file can be found here.</p>"},{"location":"learning/Install_as_package/#step-3-add-an-__init__py-file","title":"Step 3: Add an <code>__init__.py</code> File","text":"<p>Inside your package directory (<code>src/your_package_name/</code>), make sure there's an <code>__init__.py</code> file. This file can be empty, but its presence indicates to Python that this directory should be treated as a package.</p>"},{"location":"learning/Install_as_package/#step-4-build-your-package","title":"Step 4: Build Your Package","text":"<p>With your code properly organized and your <code>pyproject.toml</code> in place, you can now build your package. First, ensure you have the required tools installed:</p> <pre><code>pip install setuptools wheel\n</code></pre> <p>Navigate to your project root directory and run:</p> <pre><code>python -m build\n</code></pre> <p>This command generates distribution files in the <code>dist/</code> directory.</p>"},{"location":"learning/Install_as_package/#step-5-publish-your-package-optional","title":"Step 5: Publish Your Package (Optional)","text":"<p>If you want to upload your package to the Python Package Index, the first thing you\u2019ll need to do is register an account on TestPyPI, which is a separate instance of the package index intended for testing and experimentation. It\u2019s great for things like this tutorial where we don\u2019t necessarily want to upload to the real index. To register an account, go to https://test.pypi.org/account/register/ and complete the steps on that page. You will also need to verify your email address before you\u2019re able to upload any packages. For more details, see Using TestPyPI.</p> <p>To securely upload your project, you\u2019ll need a PyPI API token. Create one at https://test.pypi.org/manage/account/#api-tokens, setting the \u201cScope\u201d to \u201cEntire account\u201d. Don\u2019t close the page until you have copied and saved the token \u2014 you won\u2019t see that token again.</p> <p>Now that you are registered, you can use twine to upload the distribution packages. You\u2019ll need to install Twine:</p> <pre><code>pip install twine\n</code></pre> <p>Then, upload your package by running:</p> <pre><code>twine upload dist/*\n</code></pre> <p>You'll need a PyPI account and to follow the prompts to authenticate.</p>"},{"location":"learning/Install_as_package/#step-6-install-your-package","title":"Step 6: Install Your Package","text":"<p>It's useful for development and testing purposes. Navigate to the root directory of your project (where <code>setup.py</code> or <code>pyproject.toml</code> is located) and run:</p> <pre><code>pip install .\n</code></pre> <p>Or if you're actively developing the package and want changes in the package to be immediately reflected without needing to reinstall, use:</p> <pre><code>pip install -e .\n</code></pre> <p>The <code>-e</code> flag installs the package in \"editable\" mode.</p>"},{"location":"learning/Install_as_package/#additional-considerations","title":"Additional Considerations","text":"<ul> <li>Dependencies: If your package has dependencies listed in <code>pyproject.toml</code>, they will be automatically installed by <code>pip</code> during the installation process.</li> <li>Virtual Environment: It's a good practice to install your package in a virtual environment to avoid conflicts with system-wide packages. You can create a virtual environment using <code>python -m venv env</code> and activate it with <code>source env/bin/activate</code> (on Unix/macOS) or <code>env\\Scripts\\activate</code> (on Windows).</li> <li>Uninstalling: You can uninstall your package at any time with <code>pip uninstall your_package_name</code>.</li> </ul> <p>By following these methods, you can easily install your Python package locally for development, testing, or personal use without the need to publish it to PyPI.</p>"},{"location":"learning/Learn_template_tricks/","title":"Learn Template Tricks","text":"Override any config parameter from command line <pre><code>python train.py trainer.max_epochs=20 model.optimizer.lr=1e-4\n</code></pre>  **Note**: You can also add new parameters with `+` sign.  <pre><code>python train.py +model.new_param=\"owo\"\n</code></pre> Train on CPU, GPU, multi-GPU and TPU <pre><code># train on CPU\npython train.py trainer=cpu\n\n# train on 1 GPU\npython train.py trainer=gpu\n\n# train on TPU\npython train.py +trainer.tpu_cores=8\n\n# train with DDP (Distributed Data Parallel) (4 GPUs)\npython train.py trainer=ddp trainer.devices=4\n\n# train with DDP (Distributed Data Parallel) (8 GPUs, 2 nodes)\npython train.py trainer=ddp trainer.devices=4 trainer.num_nodes=2\n\n# simulate DDP on CPU processes\npython train.py trainer=ddp_sim trainer.devices=2\n\n# accelerate training on mac\npython train.py trainer=mps\n</code></pre> Train with mixed precision <pre><code># train with pytorch native automatic mixed precision (AMP)\npython train.py trainer=gpu +trainer.precision=16\n</code></pre> Train model with any logger available in PyTorch Lightning, like W&amp;B or Tensorboard <pre><code># set project and entity names in `configs/logger/wandb`\nwandb:\n  project: \"your_project_name\"\n  entity: \"your_wandb_team_name\"\n</code></pre> <pre><code># train model with Weights&amp;Biases (link to wandb dashboard should appear in the terminal)\npython train.py logger=wandb\n</code></pre>  **Note**: Lightning provides convenient integrations with most popular logging frameworks. Learn more [here](#experiment-tracking).  **Note**: Using wandb requires you to [setup account](https://www.wandb.com/) first. After that just complete the config as below.  **Note**: Click [here](https://wandb.ai/hobglob/template-dashboard/) to see example wandb dashboard generated with this template.   Train model with chosen experiment config <pre><code>python train.py experiment=example\n</code></pre>  **Note**: Experiment configs are placed in [configs/experiment/](configs/experiment/).   Attach some callbacks to run <pre><code>python train.py callbacks=default\n</code></pre>  **Note**: Callbacks can be used for things such as as model checkpointing, early stopping and [many more](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks).  **Note**: Callbacks configs are placed in [configs/callbacks/](configs/callbacks/).   Use different tricks available in Pytorch Lightning <pre><code># gradient clipping may be enabled to avoid exploding gradients\npython train.py +trainer.gradient_clip_val=0.5\n\n# run validation loop 4 times during a training epoch\npython train.py +trainer.val_check_interval=0.25\n\n# accumulate gradients\npython train.py +trainer.accumulate_grad_batches=10\n\n# terminate training after 12 hours\npython train.py +trainer.max_time=\"00:12:00:00\"\n</code></pre>  **Note**: PyTorch Lightning provides about [40+ useful trainer flags](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags).   Easily debug <pre><code># runs 1 epoch in default debugging mode\n# changes logging directory to `logs/debugs/...`\n# sets level of all command line loggers to 'DEBUG'\n# enforces debug-friendly configuration\npython train.py debug=default\n\n# run 1 train, val and test loop, using only 1 batch\npython train.py debug=fdr\n\n# print execution time profiling\npython train.py debug=profiler\n\n# try overfitting to 1 batch\npython train.py debug=overfit\n\n# raise exception if there are any numerical anomalies in tensors, like NaN or +/-inf\npython train.py +trainer.detect_anomaly=true\n\n# use only 20% of the data\npython train.py +trainer.limit_train_batches=0.2 \\\n+trainer.limit_val_batches=0.2 +trainer.limit_test_batches=0.2\n</code></pre>  **Note**: Visit [configs/debug/](configs/debug/) for different debugging configs.   Resume training from checkpoint <pre><code>python train.py ckpt_path=\"/path/to/ckpt/name.ckpt\"\n</code></pre>  **Note**: Checkpoint can be either path or URL.  **Note**: Currently loading ckpt doesn't resume logger experiment, but it will be supported in future Lightning release.   Evaluate checkpoint on test dataset <pre><code>python eval.py ckpt_path=\"/path/to/ckpt/name.ckpt\"\n</code></pre>  **Note**: Checkpoint can be either path or URL.   Create a sweep over hyperparameters <pre><code># this will run 6 experiments one after the other,\n# each with different combination of batch_size and learning rate\npython train.py -m data.batch_size=32,64,128 model.lr=0.001,0.0005\n</code></pre>  **Note**: Hydra composes configs lazily at job launch time. If you change code or configs after launching a job/sweep, the final composed configs might be impacted.   Create a sweep over hyperparameters with Optuna <pre><code># this will run hyperparameter search defined in `configs/hparams_search/mnist_optuna.yaml`\n# over chosen experiment config\npython train.py -m hparams_search=mnist_optuna experiment=example\n</code></pre>  **Note**: Using [Optuna Sweeper](https://hydra.cc/docs/next/plugins/optuna_sweeper) doesn't require you to add any boilerplate to your code, everything is defined in a [single config file](configs/hparams_search/mnist_optuna.yaml).  **Warning**: Optuna sweeps are not failure-resistant (if one job crashes then the whole sweep crashes).   Execute all experiments from folder <pre><code>python train.py -m 'experiment=glob(*)'\n</code></pre>  **Note**: Hydra provides special syntax for controlling behavior of multiruns. Learn more [here](https://hydra.cc/docs/next/tutorials/basic/running_your_app/multi-run). The command above executes all experiments from [configs/experiment/](configs/experiment/).   Execute run for multiple different seeds <pre><code>python train.py -m seed=1,2,3,4,5 trainer.deterministic=True logger=csv tags=[\"benchmark\"]\n</code></pre>  **Note**: `trainer.deterministic=True` makes pytorch more deterministic but impacts the performance.   Use Hydra tab completion  **Note**: Hydra allows you to autocomplete config argument overrides in shell as you write them, by pressing `tab` key. Read the [docs](https://hydra.cc/docs/tutorials/basic/running_your_app/tab_completion).   Run tests <pre><code># run all tests\npytest\n\n# run tests from specific file\npytest tests/test_train.py\n\n# run all tests except the ones marked as slow\npytest -k \"not slow\"\n</code></pre> Use tags  Each experiment should be tagged in order to easily filter them across files or in logger UI:  <pre><code>python train.py tags=[\"mnist\",\"experiment_X\"]\n</code></pre>  **Note**: You might need to escape the bracket characters in your shell with `python train.py tags=\\[\"mnist\",\"experiment_X\"\\]`.  If no tags are provided, you will be asked to input them from command line:  <pre><code>&gt;&gt;python train.py tags=[]\n[2022-07-11 15:40:09,358][src.utils.utils][INFO] - Enforcing tags! &lt;cfg.extras.enforce_tags=True&gt;\n[2022-07-11 15:40:09,359][src.utils.rich_utils][WARNING] - No tags provided in config. Prompting user to input tags...\nEnter a list of comma separated tags (dev):\n</code></pre>  If no tags are provided for multirun, an error will be raised:  <pre><code>&gt;&gt;python train.py -m +x=1,2,3 tags=[]\nValueError: Specify tags before launching a multirun!\n</code></pre>"},{"location":"learning/Learning_about_einops/","title":"Learn Einops: A More Readable Way to Manipulate Tensors in Python","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom einops import rearrange\n\n# Create a 2D array (4x4)\nx = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\nx = x.astype(np.float32)\n\n# Reshape to (2x8) using einops\nx_reshaped = rearrange(x, \"(h w1) (w2) -&gt; h (w1 w2)\", h=2, w1=2, w2=4)\nprint(x_reshaped)\n</pre> import numpy as np from einops import rearrange  # Create a 2D array (4x4) x = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) x = x.astype(np.float32)  # Reshape to (2x8) using einops x_reshaped = rearrange(x, \"(h w1) (w2) -&gt; h (w1 w2)\", h=2, w1=2, w2=4) print(x_reshaped) <p>The same operation can be performed in NumPy using the <code>reshape</code> method:</p> <pre># Constants\nh, w1, w2 = 2, 2, 4\n# Reshape and transpose using NumPy\nx_reshaped = x.reshape(h, w1, w2).transpose(0, 1, 2).reshape(h, w1 * w2)\n</pre> <p>But as you can see, the code is not very readable compared to the Einops version.</p> In\u00a0[\u00a0]: Copied! <pre>from einops import rearrange\n\n# Transpose a 2D array\nx_transposed = rearrange(x, \"h w -&gt; w h\")\nprint(x_transposed)\n</pre> from einops import rearrange  # Transpose a 2D array x_transposed = rearrange(x, \"h w -&gt; w h\") print(x_transposed) In\u00a0[\u00a0]: Copied! <pre>from einops import reduce\n\n# Reduce dimensions by taking the mean\nx_mean = reduce(x, \"h w -&gt; h\", \"mean\")\nprint(x_mean)\n</pre> from einops import reduce  # Reduce dimensions by taking the mean x_mean = reduce(x, \"h w -&gt; h\", \"mean\") print(x_mean) In\u00a0[\u00a0]: Copied! <pre>from einops import repeat\n\n# Repeat the array along a new dimension\nx_repeated = repeat(x, \"h w -&gt; h w c\", c=3)\nprint(x_repeated.shape)\n</pre> from einops import repeat  # Repeat the array along a new dimension x_repeated = repeat(x, \"h w -&gt; h w c\", c=3) print(x_repeated.shape) In\u00a0[\u00a0]: Copied! <pre># Combine and split dimensions\nx_combined_split = rearrange(x, \"(h1 h2) (w1 w2) -&gt; (h1 w1) (h2 w2)\", h1=2, h2=2, w1=2, w2=2)\nprint(x_combined_split)\n</pre> # Combine and split dimensions x_combined_split = rearrange(x, \"(h1 h2) (w1 w2) -&gt; (h1 w1) (h2 w2)\", h1=2, h2=2, w1=2, w2=2) print(x_combined_split) <p>In the same way as before, you can also do the operation in NumPy:</p> <pre># Constants\nh1, h2, w1, w2 = 2, 2, 2, 2\n\n# Reshape, transpose, and reshape using NumPy\nx_combined_split = x.reshape(h1, h2, w1, w2).transpose(0, 2, 1, 3).reshape(h1 * w1, h2 * w2)\n</pre> <p>But again, the code is not very readable compared to the Einops version.</p> In\u00a0[\u00a0]: Copied! <pre># Create a batch of 2D arrays\nbatch = np.array([x, x])\n\n# Apply operation on each element in the batch\nbatch_processed = rearrange(batch, \"b h w -&gt; b (h w)\")\nprint(batch_processed.shape)\n</pre> # Create a batch of 2D arrays batch = np.array([x, x])  # Apply operation on each element in the batch batch_processed = rearrange(batch, \"b h w -&gt; b (h w)\") print(batch_processed.shape) In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom einops.layers.torch import Rearrange\n\n# PyTorch example\nx_torch = torch.tensor(x)\nlayer = Rearrange(\"h w -&gt; h w 1\")\nx_torch_reshaped = layer(x_torch)\nprint(x_torch_reshaped.shape)\n</pre> import torch from einops.layers.torch import Rearrange  # PyTorch example x_torch = torch.tensor(x) layer = Rearrange(\"h w -&gt; h w 1\") x_torch_reshaped = layer(x_torch) print(x_torch_reshaped.shape)"},{"location":"learning/Learning_about_einops/#learn-einops-a-more-readable-way-to-manipulate-tensors-in-python","title":"Learn Einops: A More Readable Way to Manipulate Tensors in Python\u00b6","text":"<p>Einops (short for Einstein Operations) is a powerful Python library that provides a more readable and expressive way to manipulate tensors in deep learning and scientific computing. It's compatible with various tensor libraries like NumPy, PyTorch, and TensorFlow. In this tutorial, we'll explore how to use Einops for tensor operations.</p>"},{"location":"learning/Learning_about_einops/#1-introduction-to-einops","title":"1. Introduction to Einops\u00b6","text":"<p>Einops offers a more flexible and intuitive approach to tensor operations, such as reshaping, transposing, and repeating, by using a readable and concise syntax. It simplifies complex operations and makes your code more maintainable.</p>"},{"location":"learning/Learning_about_einops/#2-installing-einops","title":"2. Installing Einops\u00b6","text":"<p>First, you need to install Einops. You can do this using pip:</p> <pre>pip install einops\n</pre>"},{"location":"learning/Learning_about_einops/#3-basic-operations-with-einops","title":"3. Basic Operations with Einops\u00b6","text":""},{"location":"learning/Learning_about_einops/#reshape-tensors","title":"Reshape Tensors\u00b6","text":"<p>Reshaping is a common operation in tensor manipulation. With Einops, you can reshape tensors in a more intuitive way.</p>"},{"location":"learning/Learning_about_einops/#transpose-and-permute-dimensions","title":"Transpose and Permute Dimensions\u00b6","text":"<p>Transposing dimensions is simplified with Einops, allowing you to specify the new order of dimensions directly.</p>"},{"location":"learning/Learning_about_einops/#reducing-dimensions","title":"Reducing Dimensions\u00b6","text":"<p>Einops also simplifies dimensionality reduction operations like sum, mean, or max across specified axes.</p>"},{"location":"learning/Learning_about_einops/#4-advanced-operations","title":"4. Advanced Operations\u00b6","text":""},{"location":"learning/Learning_about_einops/#repeating-and-tiling-tensors","title":"Repeating and Tiling Tensors\u00b6","text":"<p>With Einops, you can easily repeat or tile tensors in any dimension.</p>"},{"location":"learning/Learning_about_einops/#combining-and-splitting-dimensions","title":"Combining and Splitting Dimensions\u00b6","text":"<p>Einops allows for complex reshaping, combining, and splitting of dimensions in a single operation.</p>"},{"location":"learning/Learning_about_einops/#working-with-batches","title":"Working with Batches\u00b6","text":"<p>Einops seamlessly integrates with batched data, a common scenario in machine learning.</p>"},{"location":"learning/Learning_about_einops/#5-integrating-with-pytorch-and-tensorflow","title":"5. Integrating with PyTorch and TensorFlow\u00b6","text":"<p>Einops works well with PyTorch and TensorFlow, allowing you to integrate its operations into deep learning models.</p>"},{"location":"learning/Learning_about_einops/#conclusion","title":"Conclusion\u00b6","text":"<p>Einops provides an elegant and powerful way to handle tensor operations, making your code more readable and concise. It's highly versatile and can be integrated with popular tensor libraries, enhancing the way you perform tensor manipulations in Python. To delve deeper into Einops, visit the official documentation.</p>"},{"location":"learning/Learning_about_hydra/","title":"Learn Hydra: A manable way to handle complex configurations","text":"In\u00a0[8]: Copied! <pre>import hydra\nfrom hydra import compose, initialize\nfrom omegaconf import OmegaConf\n\n# context initialization\nwith initialize(config_path=\"./\", job_name=\"test_app\", version_base=\"1.2\"):\n    # With overrides you can change the config values or add new ones\n    cfg = compose(config_name=\"hydra_example\", overrides=[\"+mlp.dropout=0.5\", \"mlp.in_channels=150\"])\n    print(OmegaConf.to_yaml(cfg))\n</pre> import hydra from hydra import compose, initialize from omegaconf import OmegaConf  # context initialization with initialize(config_path=\"./\", job_name=\"test_app\", version_base=\"1.2\"):     # With overrides you can change the config values or add new ones     cfg = compose(config_name=\"hydra_example\", overrides=[\"+mlp.dropout=0.5\", \"mlp.in_channels=150\"])     print(OmegaConf.to_yaml(cfg)) <pre>learning_rate: 0.1\nbatch_size: 32\nmlp:\n  _target_: torchvision.ops.MLP\n  in_channels: 150\n  hidden_channels:\n  - 100\n  dropout: 0.5\n\n</pre> In\u00a0[9]: Copied! <pre># You can now instantiate your model\nmodel = hydra.utils.instantiate(cfg.mlp)\nmodel\n</pre> # You can now instantiate your model model = hydra.utils.instantiate(cfg.mlp) model Out[9]: <pre>MLP(\n  (0): Linear(in_features=150, out_features=100, bias=True)\n  (1): Dropout(p=0.5, inplace=False)\n)</pre>"},{"location":"learning/Learning_about_hydra/#learn-hydra-a-manable-way-to-handle-complex-configurations","title":"Learn Hydra: A manable way to handle complex configurations\u00b6","text":"<p>Hydra is a powerful tool developed by Facebook Research for managing complex configurations in applications, including machine learning projects. Here's a step-by-step tutorial to get you started:</p>"},{"location":"learning/Learning_about_hydra/#1-introduction-to-hydra","title":"1. Introduction to Hydra\u00b6","text":"<p>Hydra is a framework for elegantly configuring complex applications. It allows you to create dynamic configuration files that are easy to update and maintain. This is particularly useful in machine learning, where you might need to experiment with different configurations. In our setup, every configuration file is meticulously organized within the <code>configs</code> folder, where each module is allocated its distinct subfolder. This structured approach not only segregates the configuration files but also ensures clarity and ease of navigation through our configuration setup.</p>"},{"location":"learning/Learning_about_hydra/#2-setting-up-hydra","title":"2. Setting Up Hydra\u00b6","text":"<p>First, you need to install Hydra. You can do this using pip:</p> <pre>pip install hydra-core\n</pre>"},{"location":"learning/Learning_about_hydra/#3-basic-configuration-file","title":"3. Basic Configuration File\u00b6","text":"<p>Hydra uses YAML files for configuration. Here's a simple example:</p> <pre># config.yaml\nmodel:\n  name: \"linear_regression\"\n  learning_rate: 0.01\n</pre>"},{"location":"learning/Learning_about_hydra/#4-integrating-hydra-into-your-python-script","title":"4. Integrating Hydra into Your Python Script\u00b6","text":"<p>To use Hydra, you need to decorate your main function with <code>@hydra.main()</code> and specify the path to your configuration file.</p> <pre>import hydra\n\n@hydra.main(version_base=\"1.2\", config_path=\"configs\", config_name=\"train.yaml\")\ndef main(cfg):\n    print(f\"Model: {cfg.model.name}\")\n    print(f\"Learning Rate: {cfg.model.learning_rate}\")\n\nif __name__ == \"__main__\":\n    main()\n</pre>"},{"location":"learning/Learning_about_hydra/#5-running-your-script","title":"5. Running Your Script\u00b6","text":"<p>Run your script using the Python command. Hydra will automatically read your <code>config.yaml</code> file:</p> <pre>python your_script.py\n</pre>"},{"location":"learning/Learning_about_hydra/#6-overriding-configuration-from-the-command-line","title":"6. Overriding Configuration from the Command Line\u00b6","text":"<p>One of Hydra's strengths is allowing you to override configuration parameters from the command line. For example:</p> <pre>python your_script.py model.name=svm model.learning_rate=0.001\n</pre>"},{"location":"learning/Learning_about_hydra/#7-creating-hierarchical-configurations","title":"7. Creating Hierarchical Configurations\u00b6","text":"<p>Hydra supports hierarchical configurations, which is useful for complex projects. You can split your configuration into multiple files and directories.</p>"},{"location":"learning/Learning_about_hydra/#8-advanced-features","title":"8. Advanced Features\u00b6","text":"<ul> <li>Multirun: Useful for running experiments with different configurations.</li> <li>Variable Interpolation: To dynamically set configuration values.</li> <li>Composition: Combining multiple configurations.</li> </ul>"},{"location":"learning/Learning_about_hydra/#9-best-practices","title":"9. Best Practices\u00b6","text":"<ul> <li>Keep your configurations modular.</li> <li>Use Hydra's logging capabilities.</li> <li>Utilize Hydra's powerful plugin system for more advanced scenarios.</li> </ul>"},{"location":"learning/Learning_about_hydra/#conclusion","title":"Conclusion\u00b6","text":"<p>Hydra is a powerful tool for managing configurations in machine learning projects. It simplifies experimenting with different parameters and models, making your workflow more efficient and organized.</p> <p>Remember, Hydra is continuously evolving, so always check the official documentation for the latest features and best practices.</p>"},{"location":"learning/Learning_about_hydra/#exploration","title":"Exploration\u00b6","text":"<p>Now that you've learned the basics of Hydra, try experimenting with the toy configuration file to get a better understanding of Hydra's capabilities. We are working on notebook so that you can try out Hydra in a Jupyter environment. However, keep in mind that Hydra is a command-line tool, so you don't have access to all of its features in a notebook.</p>"},{"location":"learning/Learning_about_lightning/","title":"Learn PyTorch Lightning: A Lightweight PyTorch Wrapper","text":"In\u00a0[8]: Copied! <pre>import pytorch_lightning as pl\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass LitModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.layer = nn.Linear(28 * 28, 10)\n\n    def forward(self, x):\n        return self.layer(x.view(x.size(0), -1))\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        return loss\n\n    # Uncomment to add validation step\n    # def validation_step(self, batch, batch_idx):\n    #     x, y = batch\n    #     logits = self(x)\n    #     loss = F.cross_entropy(logits, y)\n    #     # Add logging\n    #     self.log('val_loss', loss)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.001)\n\n\nmodel = LitModel()\n</pre> import pytorch_lightning as pl import torch from torch import nn from torch.nn import functional as F   class LitModel(pl.LightningModule):     def __init__(self):         super().__init__()         self.layer = nn.Linear(28 * 28, 10)      def forward(self, x):         return self.layer(x.view(x.size(0), -1))      def training_step(self, batch, batch_idx):         x, y = batch         logits = self(x)         loss = F.cross_entropy(logits, y)         return loss      # Uncomment to add validation step     # def validation_step(self, batch, batch_idx):     #     x, y = batch     #     logits = self(x)     #     loss = F.cross_entropy(logits, y)     #     # Add logging     #     self.log('val_loss', loss)      def configure_optimizers(self):         return torch.optim.Adam(self.parameters(), lr=0.001)   model = LitModel() In\u00a0[\u00a0]: Copied! <pre>from torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Define data loaders\ntrain_loader = DataLoader(\n    datasets.MNIST(\"\", train=True, download=True, transform=transforms.ToTensor()), batch_size=32, shuffle=True\n)\n\n# Check data\nx, y = next(iter(train_loader))\nprint(x.shape, y.shape)\n</pre> from torch.utils.data import DataLoader from torchvision import datasets, transforms  # Define data loaders train_loader = DataLoader(     datasets.MNIST(\"\", train=True, download=True, transform=transforms.ToTensor()), batch_size=32, shuffle=True )  # Check data x, y = next(iter(train_loader)) print(x.shape, y.shape) In\u00a0[\u00a0]: Copied! <pre># Initialize our model\nmodel = LitModel()\nprint(model)\n\n# Initialize a trainer\ntrainer = pl.Trainer(max_epochs=3)\n\n# Train the model\ntrainer.fit(model, train_loader)\n</pre> # Initialize our model model = LitModel() print(model)  # Initialize a trainer trainer = pl.Trainer(max_epochs=3)  # Train the model trainer.fit(model, train_loader) In\u00a0[\u00a0]: Copied! <pre>from pytorch_lightning.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\", dirpath=\"./my_model\", filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\"\n)\n\ntrainer = pl.Trainer(max_epochs=3, callbacks=[checkpoint_callback])\n</pre> from pytorch_lightning.callbacks import ModelCheckpoint  checkpoint_callback = ModelCheckpoint(     monitor=\"val_loss\", dirpath=\"./my_model\", filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\" )  trainer = pl.Trainer(max_epochs=3, callbacks=[checkpoint_callback])"},{"location":"learning/Learning_about_lightning/#learn-pytorch-lightning-a-lightweight-pytorch-wrapper","title":"Learn PyTorch Lightning: A Lightweight PyTorch Wrapper\u00b6","text":"<p>PyTorch Lightning is a lightweight PyTorch wrapper that helps organize your PyTorch code, making it more readable and maintainable. You can read about how to convert your PyTorch code: https://lightning.ai/docs/pytorch/stable/starter/converting.html. It abstracts away much of the boilerplate code, allowing you to focus on the core logic of your models. This tutorial will guide you through the basics of PyTorch Lightning.</p>"},{"location":"learning/Learning_about_lightning/#1-introduction-to-pytorch-lightning","title":"1. Introduction to PyTorch Lightning\u00b6","text":"<p>PyTorch Lightning separates the research code from the engineering code, helping you write scalable and more readable code. It automates most of the training loop and other common functionalities, making it easier to replicate results and scale your projects.</p>"},{"location":"learning/Learning_about_lightning/#2-installing-pytorch-lightning","title":"2. Installing PyTorch Lightning\u00b6","text":"<p>Before you begin, you need to have PyTorch installed. Then, install PyTorch Lightning via pip:</p> <pre>pip install pytorch-lightning\n</pre>"},{"location":"learning/Learning_about_lightning/#3-creating-a-lightning-module","title":"3. Creating a Lightning Module\u00b6","text":"<p>A Lightning Module is where you define your model, just like a standard PyTorch <code>nn.Module</code>, but you also define the training step, validation step, etc. Here's a simple example:</p>"},{"location":"learning/Learning_about_lightning/#4-data-preparation","title":"4. Data Preparation\u00b6","text":"<p>PyTorch Lightning works with the standard PyTorch DataLoader. Let's load the MNIST dataset as an example:</p>"},{"location":"learning/Learning_about_lightning/#5-training-the-model","title":"5. Training the Model\u00b6","text":"<p>Training a model with PyTorch Lightning is straightforward. You just need to initialize a <code>Trainer</code> and call the <code>fit</code> method:</p>"},{"location":"learning/Learning_about_lightning/#6-validation-and-testing","title":"6. Validation and Testing\u00b6","text":"<p>You can easily add validation and test steps in your <code>LitModel</code>. For validation, implement the <code>validation_step</code> method:</p> <pre>class LitModel(pl.LightningModule):\n    # ...\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        # Add logging\n        self.log('val_loss', loss)\n</pre> <p>Use a similar approach for the <code>test_step</code>.</p>"},{"location":"learning/Learning_about_lightning/#7-logging-and-callbacks","title":"7. Logging and Callbacks\u00b6","text":"<p>PyTorch Lightning comes with built-in support for logging and callbacks. You can use TensorBoard, or other loggers like MLFlow, Comet, etc.</p>"},{"location":"learning/Learning_about_lightning/#8-advanced-features","title":"8. Advanced Features\u00b6","text":"<p>PyTorch Lightning also supports distributed training, mixed precision training, and more. These features can be easily activated in the Trainer (make sure you have the required hardware and software installed). Here for example, we would start training on two GPUs and use mixed precision training:</p> <pre>trainer = pl.Trainer(gpus=2, precision=16)\n</pre> <p>If you don't know the number of GPUs available, you can set <code>gpus=-1</code> and PyTorch Lightning will automatically use all available GPUs.</p> <p>For more options on the <code>Trainer</code>, check out the documentation.</p>"},{"location":"learning/Learning_about_lightning/#conclusion","title":"Conclusion\u00b6","text":"<p>PyTorch Lightning is a powerful tool for organizing PyTorch code and making it more efficient and maintainable. It abstracts away the engineering details, allowing you to focus on the research part. This tutorial covered the basics, but there's a lot more to explore, including advanced features like distributed training, and integrations with other tools and libraries. Be sure to check out the official documentation for more information.</p>"},{"location":"learning/QuickStart/","title":"Quick Start Guide for New Projects","text":""},{"location":"learning/QuickStart/#0-prepare-the-environment","title":"0. Prepare the Environment","text":"<ul> <li>Set up your environment:   <pre><code>conda create -n myenv python=3.9\nconda activate myenv\npip install -r requirements.txt\n</code></pre></li> </ul>"},{"location":"learning/QuickStart/#1-project-initialization","title":"1. Project Initialization","text":"<ul> <li>Use the Minimal Lightning Hydra Template: Click \"Use this template\" on GitHub, or clone and remove <code>.git</code> for a fresh start:   <pre><code>git clone https://github.com/antonibigata/minimal-lightning-hydra-template.git &lt;project-name&gt;\ncd &lt;project-name&gt;\nrm -rf .git\n</code></pre></li> </ul>"},{"location":"learning/QuickStart/#2-data-setup","title":"2. Data Setup","text":"<ul> <li>Structure data code in <code>src/datamodules</code> and <code>components/dataset.py</code>.</li> <li>Configure in <code>configs/datamodule/datamodule.yaml</code>; use <code>default.yaml</code> for shared parameters.</li> <li>Example: Adapt FashionMNIST dataset and configurations. Implement and test custom datamodules and datasets.</li> </ul>"},{"location":"learning/QuickStart/#3-model-and-network-configuration","title":"3. Model and Network Configuration","text":"<ul> <li>Define networks in <code>src/models/components/nets</code> and configure in <code>configs/model/net</code>.</li> <li>Create simple models for practice, referencing provided examples. Write unit tests to ensure functionality.</li> </ul>"},{"location":"learning/QuickStart/#4-model-creation","title":"4. Model Creation","text":"<ul> <li>Develop your model in <code>src/models</code> as a PyTorch Lightning <code>LightningModule</code>.</li> <li>Use <code>configs/model/default.yaml</code> for base settings; add specific model configs as needed.</li> <li>Examine example models and consult PyTorch Lightning documentation for guidance.</li> </ul>"},{"location":"learning/QuickStart/#5-training-setup","title":"5. Training Setup","text":"<ul> <li>Manage training with PyTorch Lightning <code>Trainer</code> and configurations in <code>configs/trainer</code>.</li> <li>Adjust training settings via command line, e.g., <code>python src/train.py trainer.max_epochs=20</code>.</li> <li>Use <code>configs/train.yaml</code> to encompass datamodule, model, callbacks, logger, trainer, and paths settings.</li> </ul>"},{"location":"learning/QuickStart/#6-launch-training","title":"6. Launch Training","text":"<ul> <li>Start with <code>python src/train.py</code>. Override configs as required, e.g., <code>python src/train.py model/net=conv_net</code>.</li> <li>For GPU training, add <code>trainer=gpu</code>.</li> </ul>"},{"location":"learning/QuickStart/#7-logging-with-wandb","title":"7. Logging with Wandb","text":"<ul> <li>Install Wandb, sign up, and authenticate. Modify <code>configs/train.yaml</code> for Wandb logging or use <code>python src/train.py logger=wandb</code>.</li> <li>Log training metrics, images, and explore advanced Wandb features as per documentation.</li> </ul>"},{"location":"learning/QuickStart/#8-evaluation","title":"8. Evaluation","text":"<ul> <li>Evaluate model performance with <code>python src/eval.py ckpt_path=&lt;path_to_checkpoint&gt;</code>.</li> <li>Use test set for unbiased assessment; locate checkpoints in <code>logs/train/runs</code>.</li> </ul>"},{"location":"learning/QuickStart/#9-inference","title":"9. Inference","text":"<ul> <li>Perform inference on custom data: <code>python src/inference.py ckpt_path=&lt;checkpoint&gt; image_path=&lt;image_path&gt;</code>.</li> <li>Repository includes sample FashionMNIST images in <code>logs/data_check</code> for quick tests.</li> </ul>"},{"location":"learning/Starting_a_new_project/","title":"Steps When Starting a New Project","text":"<p>For a concise version of this tutorial, please refer to the QuickStart.md file.</p>"},{"location":"learning/Starting_a_new_project/#0-prepare-the-environment","title":"0. Prepare the Environment","text":"<p>Before starting a new project, ensure that you have the necessary tools and libraries installed. This includes Python, PyTorch, PyTorch Lightning, Hydra, and Wandb. You can install these libraries using the following commands:</p>"},{"location":"learning/Starting_a_new_project/#via-conda","title":"Via Conda","text":"<pre><code># Create conda environment\nconda create -n myenv python=3.9\nconda activate myenv\n\n# Install requirements\npip install -r requirements.txt\n</code></pre>"},{"location":"learning/Starting_a_new_project/#via-venv","title":"Via Venv","text":"<pre><code># Create virtual environment\npython -m venv myenv\nsource myenv/bin/activate\n\n# Install requirements\npip install -r requirements.txt\n</code></pre>"},{"location":"learning/Starting_a_new_project/#1-create-a-new-project-based-on-the-template","title":"1. Create a New Project Based on the Template:","text":"<p>To start a new project, it's beneficial to use a template that provides a structured foundation for your work. This template should include a well-organized directory structure, essential configuration files, and a set of example components, such as datamodules, models, and training scripts. This approach streamlines the project setup process and ensures that you adhere to best practices from the outset.</p> <p>You can start by using the template available at Minimal Lightning Hydra Template. To create a new project from this template: - Navigate to the GitHub page of the template. - Click on the \"Use this template\" button, located in the top right corner. - Follow the on-screen instructions to create a new repository based on this template.</p> <p>If you prefer to clone the repository: <pre><code>git clone https://github.com/antonibigata/minimal-lightning-hydra-template.git your-project-name\ncd your-project-name\nrm -rf .git\n</code></pre> This will clone the repository and remove its git history, allowing you to start a fresh project.</p> <p>Alternatively, if you just want to follow along with this tutorial, you can just clone this repository and follow the instructions.</p> <pre><code>git clone https://github.com/ImperialCollegeLondon/ReCoDE-DeepLearning-Best-Practices.git\ncd ReCoDE-DeepLearning-Best-Practices\n</code></pre>"},{"location":"learning/Starting_a_new_project/#2-create-a-datamodule-and-dataset-file-with-corresponding-configuration","title":"2. Create a Datamodule and Dataset File with Corresponding Configuration:","text":"<p>Data manipulation code should be organized in <code>src/datamodules</code>. Key components include: - Datamodule: Responsible for downloading data, and splitting it into training, validation, and testing sets. - Dataset: Handles data loading and applying transformations.</p> <p>Define these components in <code>datamodule.py</code> and <code>components/dataset.py</code>, respectively. Configure the datamodule in <code>configs/datamodule/datamodule.yaml</code>.</p> <p>For efficient experimentation with multiple datasets, use a <code>default.yaml</code> file in the <code>configs</code> folder to define common parameters like <code>batch_size</code> and <code>num_workers</code>. Create separate configuration files for each dataset by inheriting from the default configuration. This setup allows you to switch datasets easily by modifying the datamodule configuration.</p> <p>As an example, we will use the FashionMNIST dataset. I suggest writing your own datamodule and dataset files, but you can also copy the ones from this project. It is also good practice to write a small test to ensure that the datamodule and dataset are functioning as expected. Examples of these tests can be found at the end of both files.</p>"},{"location":"learning/Starting_a_new_project/#3-create-networks-with-corresponding-configuration","title":"3. Create Networks with Corresponding Configuration:","text":"<p>Your main model will rely on networks, which are essentially the your building blocks, such as layers or a set of layers that perform specific functions (like encoders and decoders in autoencoders). Define these networks in <code>src/models/components/nets</code>. Networks are versatile and can be reused across different models. For instance, an encoder network designed for one model can potentially be used in another model with similar requirements.</p> <p>Network configurations should be placed in <code>configs/model/net</code>. This maintains organization and clarity, especially when dealing with multiple networks.</p> <p>As a practical exercise, start by creating a simple model and network. Even if it's basic, this exercise will help you understand the process of building and integrating these components. Use the examples provided in this project as a reference or solution. To ensure your model and networks are set up correctly, it's advisable to write unit tests. These tests verify the functionality of each component. You can find examples of such tests at the end of the respective files in this project.</p> <p>By following these steps, you'll gain hands-on experience in setting up and configuring models and networks, which are crucial for developing effective machine learning solutions.</p>"},{"location":"learning/Starting_a_new_project/#4-create-model-and-configuration","title":"4. Create Model and Configuration:","text":"<p>In this step, you will define your main model within the <code>src/models</code> directory. This model should be a PyTorch Lightning module, which means it inherits from <code>LightningModule</code>. The model encapsulates your machine learning algorithm, including methods for the forward pass, loss calculation, and the optimizer step. It's also where you'll define the main training, validation, and testing loops.</p> <p>For configuration, similar to the datamodule, set up a default configuration in <code>configs/model/default.yaml</code>. This file should contain settings common to all models. Then, for specific models, create separate configuration files within the same directory. This approach allows for flexibility and ease in switching between different model configurations.</p> <p>To get started, you're encouraged to write your own model, but do take the time to examine the example model included in this project. It serves as a practical reference to understand the standard structure and components of a PyTorch Lightning module. Additionally, refer to the PyTorch Lightning documentation, particularly the LightningModule section at LightningModule Documentation. This resource is invaluable for understanding the different methods in a LightningModule and how they interact in a real-world scenario. By studying these examples and the documentation, you will gain a deeper insight into the efficient implementation and functionalities of your machine learning model.</p>"},{"location":"learning/Starting_a_new_project/#5-create-utils","title":"5. Create utils:","text":"<p>(according to my learning experience following this tutorial, this step should be done before train.py code can be run) please add some text to this step.</p>"},{"location":"learning/Starting_a_new_project/#6-modify-training-configuration","title":"6. Modify Training Configuration:","text":"<p>At this point, you have assembled all the essential components needed to commence the training of your model. Central to this process is the PyTorch Lightning <code>Trainer</code>, which orchestrates the training workflow. The Trainer manages crucial aspects of the training process, such as:</p> <ul> <li>Number of training epochs.</li> <li>Logging of training progress and metrics.</li> <li>Checkpointing, which involves saving the model at specific intervals or under certain conditions.</li> </ul> <p>The configuration settings for the Trainer are located in <code>configs/trainer</code>. This directory contains a set of default configurations tailored to different training scenarios. These default settings can be conveniently overridden via command-line arguments. For example, to modify the number of training epochs, you can execute a command like:</p> <pre><code>python src/train.py trainer.max_epochs=20\n</code></pre> <p>Let's dive into the <code>configs/train.yaml</code> file, which serves as the master configuration encapsulating all modules of your project:</p> <ol> <li> <p>datamodule: Specifies the datamodule configuration, as outlined in section 2. This includes details about data preprocessing, batching, and split between training, validation, and test sets.</p> </li> <li> <p>model: Contains the model configuration as discussed earlier. It defines the structure and parameters of your machine learning model.</p> </li> <li> <p>callbacks: Manages callbacks used during training. The initial setup typically includes logging callbacks, but you can activate additional callbacks like early stopping or model checkpointing by modifying <code>configs/callbacks/defaults.yaml</code>.</p> </li> <li> <p>logger: Handles the configuration for logging. More details on this will be covered in the next section. For now, it can be left as <code>null</code>.</p> </li> <li> <p>trainer: Defines trainer-specific configurations, such as the number of epochs, the number of devices (GPUs) to use, and other training-related settings.</p> </li> <li> <p>paths: This section outlines various paths used by the project, including paths to the data, logs, and checkpoints. You can stick with the default settings unless you have specific requirements.</p> </li> </ol> <p>By configuring these components in the <code>configs/train.yaml</code> file, you create a cohesive and flexible training environment. This setup allows you to easily adjust parameters and components to suit different experiments and project needs.</p>"},{"location":"learning/Starting_a_new_project/#7-start-the-training","title":"7. Start the Training:","text":"<p>Now that you have set up all the necessary components, you are ready to initiate the training of your model. Begin by executing the following command:</p> <pre><code>python src/train.py\n</code></pre> <p>This command launches the main training loop by running the <code>train.py</code> script. Within <code>train.py</code>, several key actions are performed:</p> <ul> <li>Initialization of the datamodule, which prepares your data for the training process.</li> <li>Setting up the model, tailored to your specific machine learning task.</li> <li>Configuring and starting the PyTorch Lightning Trainer, which manages the training process.</li> </ul> <p>In addition to orchestrating the training, <code>train.py</code> is also responsible for handling logging. More details about logging will be discussed in the subsequent section.</p> <p>The above command initiates training using the default configuration settings. However, Hydra's flexible design allows you to easily override configuration parameters directly from the command line. For instance, if you want to switch the network architecture within your model, you can do so with a command like:</p> <pre><code>python src/train.py model/net=conv_net\n</code></pre> <p>Note: To change an entire configuration file (as opposed to a single parameter), use <code>/</code> instead of <code>.</code> to separate configuration names.</p> <p>[OPTIONAL GPU TRAINING] If you have access to a GPU, you can significantly accelerate the training process by specifying the <code>trainer=gpu</code> argument:</p> <pre><code>python src/train.py trainer=gpu\n</code></pre> <p>This command instructs the Trainer to utilize available GPU resources, harnessing their computational power for more efficient training.</p>"},{"location":"learning/Starting_a_new_project/#8-enhanced-experiment-tracking-and-visualization","title":"8. Enhanced Experiment Tracking and Visualization:","text":"<p>Logging training progress and metrics is crucial for monitoring a model's performance throughout the training process. This functionality is configured within the <code>logger</code> section of <code>configs/train.yaml</code>. By default, the <code>logger</code> is set to <code>null</code>, indicating that no logging will occur. However, enabling logging is straightforward and highly recommended for a more insightful training experience.</p> <p>Wandb (Weights &amp; Biases) is a widely used platform for experiment tracking, visualization, and analysis. It offers a comprehensive suite of tools to log metrics, hyperparameters, outputs, and much more. To incorporate Wandb into your project:</p> <ul> <li> <ol> <li>Installation: First, ensure that the Wandb library is installed. If not, you can install it using pip:</li> </ol> </li> </ul> <pre><code>pip install wandb\n</code></pre> <ul> <li> <ol> <li>Create an Account: Visit the Wandb website to sign up for an account. Signing up is free and allows you to track and visualize your experiments in one place.</li> </ol> </li> <li> <ol> <li>Authenticate: Obtain your Wandb API key from your account settings. Then, authenticate your local environment by running:</li> </ol> </li> </ul> <pre><code>wandb login\n</code></pre> <p>This command prompts you to enter your API key, securely linking your experiments with your Wandb account.</p> <ul> <li> <ol> <li>Enable Wandb Logging: To start logging to Wandb, modify the <code>logger</code> section in <code>configs/train.yaml</code> by setting it to <code>wandb</code>:</li> </ol> </li> </ul> <pre><code>logger: wandb\n</code></pre> <p>Alternatively, you can directly activate Wandb logging via the command line when you start your training:</p> <pre><code>python src/train.py logger=wandb\n</code></pre> <p>What Can You Log?</p> <p>Wandb's flexibility allows you to log a wide array of data, from training and validation metrics to rich media such as images, videos, and even audio clips. This capability is invaluable for in-depth monitoring and analysis of your training process, offering insights that can guide model improvement.</p> <p>For detailed instructions on logging specific data types and leveraging Wandb's full potential, refer to the Wandb Documentation. Here, you'll find a treasure trove of guides and examples to enhance your logging strategy.</p> <p>Logging Images and Confusion Matrices</p> <p>In addition to the standard training and validation metrics, this tutorial also covers how to log images and confusion matrices using Wandb. Logging images can provide visual feedback on the model's performance, such as the accuracy of image classifications or the quality of generated images. Confusion matrices, on the other hand, offer a detailed view of the model's prediction accuracy across different classes, helping identify areas where the model may be struggling. Both of these logging capabilities are powerful tools for diagnosing model behavior and guiding improvements. Detailed instructions on how to implement these logging features are included, allowing you to gain deeper insights into your model's performance and make data-driven decisions to enhance its accuracy and effectiveness.</p>"},{"location":"learning/Starting_a_new_project/#9-evaluate-the-model","title":"9. Evaluate the model:","text":"<p>After completing the training phase, it's crucial to assess the model's performance using a separate test set. This step is vital for gauging the model's generalization abilities\u2014essentially, its capacity to make accurate predictions on new, unseen data. This unbiased evaluation helps in understanding how the model would perform in real-world scenarios.</p> <p>Initiating the Model Evaluation</p> <p>To start the evaluation process, use the following command:</p> <pre><code>python src/eval.py ckpt_path=PATH_TO_CHECKPOINT\n</code></pre> <p>Here, <code>PATH_TO_CHECKPOINT</code> should be replaced with the actual path to your saved model checkpoint, for example, <code>logs/train/runs/2024-02-05_10-41-42</code>. This path points to the specific model folder checkpoint you intend to evaluate. The <code>eval.py</code> script is designed to load this checkpoint and then carry out the evaluation on the designated test dataset.</p> <p>Note on Dataset Usage:</p> <p>For illustrative purposes, the evaluation script in this project is configured to run on the FashionMNIST dataset's validation set. However, for a thorough evaluation of your model, especially when working with your own datasets, it is strongly recommended to perform this assessment on a dedicated test set. The test set should consist of data that has not been used during the training or validation phases to ensure an impartial evaluation of the model's performance.</p> <p>Locating the Checkpoint File:</p> <p>The checkpoint file is typically saved in a directory structured by the training date and time, for example, <code>logs/train/runs/YYYY-MM-DD_HH-MM-SS</code>. If you're unsure about the checkpoint path, refer to the training logs or the directory where your training outputs are saved. This can help you identify the correct checkpoint file for evaluation.</p> <p>Best Practices for Evaluation:</p> <ul> <li>Ensure that the test set is properly prepared and represents the data distribution expected in real-world applications.</li> <li>Consider evaluating the model on multiple checkpoints to identify the best-performing model over different stages of training.</li> <li>Use a comprehensive set of metrics for evaluation, tailored to your specific problem, to get a holistic view of the model's performance.</li> </ul>"},{"location":"learning/Starting_a_new_project/#10-model-inference-on-custom-data","title":"10. Model Inference on Custom Data:","text":"<p>After your model has been rigorously trained and its performance thoroughly evaluated, the next step is to deploy it for practical use \u2014 a phase known as inference. Inference is the application of your trained model to make predictions or decisions based on new, unseen data. This step is critical for realizing the model's value in real-world applications, from classifying images to making recommendations.</p> <p>Performing Inference on Custom Data</p> <p>To apply your model to custom data, utilize the <code>inference.py</code> script with the following command structure:</p> <pre><code>python src/inference.py ckpt_path=PATH_TO_CHECKPOINT image_path=PATH_TO_IMAGE\n</code></pre> <ul> <li><code>PATH_TO_CHECKPOINT</code> should be substituted with the actual path to your trained model's checkpoint. This file encapsulates the learned weights that your model will use to make predictions.</li> <li><code>PATH_TO_IMAGE</code> needs to be replaced with the path to the specific image file you wish to analyze with your model.</li> </ul> <p>In this particular project, the inference script is designed to work with the FashionMNIST dataset so the image needs to be a FashionMNIST image (i.e 28x28 black and white image). However, you can easily modify the script to work with your own custom data. To facilitate immediate testing of the inference capabilities of your model, the repository includes two sample images from the FashionMNIST dataset. These images are located in logs/data_check. This inclusion allows you to quickly test the model's inference process without the need for external data. To use these for inference testing, simply replace PATH_TO_IMAGE in the inference command with the path to one of these sample images.</p> <p>The script executes two primary actions: 1. Model Loading: It begins by loading your model from the specified checkpoint. This step reconstitutes your model with all its learned parameters, readying it for prediction. 2. Prediction: Next, the script processes the provided image, applying the model to generate predictions based on the learned patterns during training.</p> <p>Practical Tips:</p> <ul> <li>If you're working with batches of images or data points rather than a single instance, consider modifying the <code>inference.py</code> script to handle batch processing for more efficiency.</li> <li>Explore different model checkpoints to observe any variations in prediction outcomes. This can help identify the most stable and reliable version of your model for deployment.</li> <li>Keep in mind the context of your application. The nature of the data and the specific requirements of your use case should guide how you approach inference, from selecting the right model to choosing the appropriate data for prediction.</li> </ul>"}]}